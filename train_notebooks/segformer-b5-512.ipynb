{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14815400,"sourceType":"datasetVersion","datasetId":9474164}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\nprint(\"GPU Memory Flushed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport multiprocessing\nimport subprocess\nimport time\nimport numpy as np\nimport evaluate\nfrom datasets import Dataset, Image as DSImage\nfrom transformers import (\n    SegformerImageProcessor,\n    SegformerForSemanticSegmentation,\n    TrainingArguments,\n    Trainer,\n    logging\n)\nfrom torch import nn\n\nlogging.set_verbosity_info()\n\nMODEL_NAME = \"nvidia/mit-b5\" \nOUTPUT_DIR = \"/kaggle/working/checkpoints_b5_512\"\nFINAL_MODEL_DIR = \"/kaggle/working/final_rat_model_b5_512\"\n\nDATASET_PATHS = [\n    (\"/kaggle/input/rodent-b5-dataset/b5-dataset/b5-processed/images\", \"/kaggle/input/rodent-b5-dataset/b5-dataset/b5-processed/masks\"),\n]\n\nEPOCHS = 30\nLEARNING_RATE = 6e-5\n\nBATCH_SIZE = 4            \nGRAD_ACCUMULATION = 4 \nGRAD_ACCUMULATION = 4     \n\n# --- 1. GPU MONITOR ---\ndef monitor_gpu(interval=60):\n    while True:\n        try:\n            result = subprocess.check_output(\n                [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n            ).decode().strip().split('\\n')\n            stats = [f\"GPU {i}: {line.split(',')[0]}% Util | {line.split(',')[1]}/{line.split(',')[2]} MB\" for i, line in enumerate(result)]\n            print(f\"\\n[GPU MONITOR] \" + \" | \".join(stats) + \"\\n\")\n        except Exception: pass\n        time.sleep(interval)\n\n# --- 2. DATA LOAD (COMBINED) ---\ndef load_dataset():\n    print(f\"--- LOADING COMBINED DATASETS ---\")\n    \n    final_image_paths = []\n    final_mask_paths = []\n\n    # Loop through paths and collect files\n    for img_dir, mask_dir in DATASET_PATHS:\n        if not os.path.exists(img_dir):\n            print(f\" Skipping missing directory: {img_dir}\")\n            continue\n            \n        print(f\"Scanning: {img_dir}...\")\n        all_images = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n        img_map = {os.path.splitext(f)[0]: f for f in all_images}\n        \n        # Find matching masks\n        valid_count = 0\n        for base_name, img_file in img_map.items():\n            # Try png first, then jpg\n            mask_name = f\"{base_name}.png\"\n            if not os.path.exists(os.path.join(mask_dir, mask_name)):\n                mask_name = f\"{base_name}.jpg\"\n            \n            if os.path.exists(os.path.join(mask_dir, mask_name)):\n                final_image_paths.append(os.path.join(img_dir, img_file))\n                final_mask_paths.append(os.path.join(mask_dir, mask_name))\n                valid_count += 1\n        print(f\"   -> Found {valid_count} pairs.\")\n\n    print(f\"--- TOTAL DATA: {len(final_image_paths)} pairs ---\")\n    \n    if len(final_image_paths) == 0:\n        raise ValueError(\"CRITICAL: No images found! Check your DATASET_PATHS.\")\n\n    ds = Dataset.from_dict({\"image\": final_image_paths, \"label\": final_mask_paths})\n    ds = ds.cast_column(\"image\", DSImage())\n    ds = ds.cast_column(\"label\", DSImage())\n    ds = ds.train_test_split(test_size=0.10, seed=42)\n    return ds\n\n# --- 3. PROCESSOR & TRANSFORMS ---\nprocessor = SegformerImageProcessor.from_pretrained(\n    MODEL_NAME, \n    reduce_labels=False,\n    do_resize=True,\n    size={\"height\": 512, \"width\": 512}\n)\n\ndef train_transforms(example_batch):\n    images = [x.convert(\"RGB\") for x in example_batch[\"image\"]]\n    labels = []\n    for x in example_batch[\"label\"]:\n        mask_np = np.array(x.convert(\"L\"))\n        mask_np = np.where(mask_np > 0, 1, 0).astype(np.uint8)\n        labels.append(mask_np)\n        \n    return processor(images, labels, return_tensors=\"pt\")\n\n# --- 4. SANITY CHECK ---\ndef sanity_check(ds):\n    print(\"--- RUNNING SANITY CHECK ---\")\n    sample = ds[\"train\"][0]\n    output = train_transforms({\"image\": [sample[\"image\"]], \"label\": [sample[\"label\"]]})\n    unique_vals = torch.unique(output[\"labels\"]).tolist()\n    print(f\"Processed Mask Values: {unique_vals}\")\n    if any(v > 1 for v in unique_vals):\n        raise ValueError(f\" CRITICAL: Mask contains values {unique_vals}. Must be only [0, 1].\")\n    print(\" DATA IS SAFE.\")\n\n# --- 5. METRICS & MODEL ---\nmetric = evaluate.load(\"mean_iou\")\nid2label = {0: \"background\", 1: \"rat\"}\nlabel2id = {\"background\": 0, \"rat\": 1}\n\ndef compute_metrics(eval_pred):\n    with torch.no_grad():\n        logits, labels = eval_pred\n        logits_tensor = torch.from_numpy(logits)\n        \n        logits_tensor = nn.functional.interpolate(\n            logits_tensor, \n            size=labels.shape[-2:], \n            mode=\"bilinear\", \n            align_corners=False\n        ).argmax(dim=1)\n\n        metrics = metric.compute(\n            predictions=logits_tensor.numpy(),\n            references=labels,\n            num_labels=2,\n            ignore_index=255,\n            reduce_labels=False,\n        )\n        return {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in metrics.items()}\n\n# --- 6. TRAINER ---\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        \n        upsampled_logits = nn.functional.interpolate(\n            logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n        \n        # Heavy weight on Class 1 (Rat) to force the model to find it\n        weights = torch.tensor([1.0, 5.0]).to(logits.device)\n        loss_fct = nn.CrossEntropyLoss(weight=weights)\n        \n        loss = loss_fct(upsampled_logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# --- 7. MAIN ---\ndef main():\n    ds = load_dataset()\n    sanity_check(ds)\n    \n    ds[\"train\"].set_transform(train_transforms)\n    ds[\"test\"].set_transform(train_transforms)\n\n    model = SegformerForSemanticSegmentation.from_pretrained(\n        MODEL_NAME,\n        num_labels=2,\n        id2label=id2label,\n        label2id=label2id,\n        ignore_mismatched_sizes=True\n    )\n\n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        learning_rate=LEARNING_RATE,\n        num_train_epochs=EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRAD_ACCUMULATION,\n        fp16=True,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"steps\",\n        save_steps=50,\n        save_total_limit=2,\n        logging_steps=10,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"mean_iou\",\n        report_to=\"none\",\n        remove_unused_columns=False\n    )\n\n    trainer = WeightedTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=ds[\"train\"],\n        eval_dataset=ds[\"test\"],\n        compute_metrics=compute_metrics,\n    )\n\n    last_checkpoint = None\n    if os.path.isdir(OUTPUT_DIR):\n        checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n        if checkpoints:\n            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n            last_checkpoint = os.path.join(OUTPUT_DIR, checkpoints[-1])\n            print(f\"!!! RESUMING FROM: {last_checkpoint} !!!\")\n\n    print(f\"--- TRAINING START: MIT-B5 @ 512x512 ---\")\n    trainer.train(resume_from_checkpoint=last_checkpoint)\n\n    print(f\"--- SAVING TO {FINAL_MODEL_DIR} ---\")\n    trainer.save_model(FINAL_MODEL_DIR)\n    processor.save_pretrained(FINAL_MODEL_DIR)\n    print(\"DONE.\")\n\nif __name__ == \"__main__\":\n    p = multiprocessing.Process(target=monitor_gpu, daemon=True)\n    p.start()\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}