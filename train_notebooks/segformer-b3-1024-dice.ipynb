{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14727750,
     "sourceType": "datasetVersion",
     "datasetId": 9410959
    }
   ],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install evaluate",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import time\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import Dataset, Image as DSImage\n",
    "from transformers import (\n",
    "    SegformerImageProcessor,\n",
    "    SegformerForSemanticSegmentation,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    logging\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_NAME = \"nvidia/mit-b3\"\n",
    "OUTPUT_DIR = \"/kaggle/working/checkpoints_b3_1024_DICE\"\n",
    "FINAL_MODEL_DIR = \"/kaggle/working/final_rat_model_b3_1024_DICE\"\n",
    "\n",
    "IMAGE_DIR = \"/kaggle/input/datasets/gonoszgonosz/rodent-data-2/processed/images\"\n",
    "MASK_DIR = \"/kaggle/input/datasets/gonoszgonosz/rodent-data-2/processed/masks\"\n",
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 6e-5\n",
    "BATCH_SIZE = 1              \n",
    "GRAD_ACCUMULATION = 16      \n",
    "\n",
    "# --- 1. GPU MONITOR ---\n",
    "def monitor_gpu(interval=60):\n",
    "    while True:\n",
    "        try:\n",
    "            result = subprocess.check_output(\n",
    "                [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used,memory.total\", \"--format=csv,noheader,nounits\"]\n",
    "            ).decode().strip().split('\\n')\n",
    "            stats = [f\"GPU {i}: {line.split(',')[0]}% Util | {line.split(',')[1]}/{line.split(',')[2]} MB\" for i, line in enumerate(result)]\n",
    "            print(f\"\\n[GPU MONITOR] \" + \" | \".join(stats) + \"\\n\")\n",
    "        except Exception: pass\n",
    "        time.sleep(interval)\n",
    "\n",
    "# --- 2. DATA LOAD ---\n",
    "def load_dataset():\n",
    "    all_images = [f for f in os.listdir(IMAGE_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "    all_masks = [f for f in os.listdir(MASK_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "    img_map = {os.path.splitext(f)[0]: f for f in all_images}\n",
    "    mask_map = {os.path.splitext(f)[0]: f for f in all_masks}\n",
    "    common_ids = sorted(list(set(img_map.keys()) & set(mask_map.keys())))\n",
    "    \n",
    "    final_image_paths = [os.path.join(IMAGE_DIR, img_map[i]) for i in common_ids]\n",
    "    final_mask_paths = [os.path.join(MASK_DIR, mask_map[i]) for i in common_ids]\n",
    "\n",
    "    ds = Dataset.from_dict({\"image\": final_image_paths, \"label\": final_mask_paths})\n",
    "    ds = ds.cast_column(\"image\", DSImage())\n",
    "    ds = ds.cast_column(\"label\", DSImage())\n",
    "    ds = ds.train_test_split(test_size=0.10, seed=42)\n",
    "    return ds\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\n",
    "    MODEL_NAME, do_resize=True, size={\"height\": 1024, \"width\": 1024} \n",
    ")\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [x.convert(\"RGB\") for x in example_batch[\"image\"]]\n",
    "    labels = []\n",
    "    for x in example_batch[\"label\"]:\n",
    "        mask_np = np.array(x.convert(\"L\"))\n",
    "        mask_np = np.where(mask_np > 0, 1, 0).astype(np.uint8)\n",
    "        labels.append(mask_np)\n",
    "    return processor(images, labels, return_tensors=\"pt\")\n",
    "\n",
    "# --- 4. DICE LOSS IMPLEMENTATION ---\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Flatten predictions and targets\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        probs = probs[:, 1, :, :].contiguous().view(-1)\n",
    "        targets = targets.contiguous().view(-1).float()\n",
    "\n",
    "        intersection = (probs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (probs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        \n",
    "        loss_fct = DiceLoss()\n",
    "        loss = loss_fct(upsampled_logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# --- 5. METRICS ---\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        metrics = metric.compute(\n",
    "            predictions=logits_tensor.numpy(),\n",
    "            references=labels,\n",
    "            num_labels=2,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        return {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in metrics.items()}\n",
    "\n",
    "# --- 6. MAIN ---\n",
    "def main():\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"REPORT_TO\"] = \"none\"\n",
    "    \n",
    "    ds = load_dataset()\n",
    "    ds[\"train\"].set_transform(train_transforms)\n",
    "    ds[\"test\"].set_transform(train_transforms)\n",
    "\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        MODEL_NAME, num_labels=2, id2label={0: \"background\", 1: \"rat\"}, label2id={\"background\": 0, \"rat\": 1}, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=6e-5,\n",
    "        num_train_epochs=30,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mean_iou\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # Note: Using the DiceTrainer here\n",
    "    trainer = DiceTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"--- TRAINING START: DICE LOSS BASELINE ---\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"--- SAVING TO {FINAL_MODEL_DIR} ---\")\n",
    "    trainer.save_model(FINAL_MODEL_DIR)\n",
    "    processor.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = multiprocessing.Process(target=monitor_gpu, daemon=True)\n",
    "    p.start()\n",
    "    main()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
