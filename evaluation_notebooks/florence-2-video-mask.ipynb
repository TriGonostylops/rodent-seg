{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14788594,"datasetId":9454374,"databundleVersionId":15642492}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers\n!pip install transformers==4.44.2 einops timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers einops timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\n\n# ================= THE CORRECTED HUGGING FACE BYPASS PATCH =================\nimport transformers.dynamic_module_utils as dyn_utils\noriginal_check = dyn_utils.check_imports\n\ndef custom_check_imports(filename):\n    try:\n        return original_check(filename)\n    except ImportError as e:\n        if \"flash_attn\" in str(e):\n            # Bypass the error and return the relative imports manually\n            return dyn_utils.get_relative_imports(filename)\n        else:\n            raise e\n\ndyn_utils.check_imports = custom_check_imports\n# =========================================================================\n\n# ================= CONFIGURATION =================\nINPUT_VIDEO = \"/kaggle/input/datasets/gonoszgonosz/rat-test-video/test.mp4\"\nOUTPUT_VIDEO = \"/kaggle/working/Florence2_Video_Output.mp4\"\n\nMODEL_ID = \"microsoft/Florence-2-large\"\nTASK_PROMPT = \"<REFERRING_EXPRESSION_SEGMENTATION>\"\nTEXT_INPUT = \" rat\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# =================================================\n\ndef apply_overlay(image, mask, color=(0, 255, 255), alpha=0.5):\n    \"\"\"Blends a solid color over the masked region (Yellow for Florence).\"\"\"\n    overlay = np.full_like(image, color)\n    blended = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)\n    res = image.copy()\n    res[mask == 1] = blended[mask == 1]\n    return res\n\ndef main():\n    print(\"--- LOADING FLORENCE-2 VLM ---\")\n    \n    # This is the line that was missing:\n    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID, torch_dtype=torch.float16, trust_remote_code=True\n    ).to(device).eval()\n\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    w, h = int(cap.get(3)), int(cap.get(4))\n    fps = cap.get(5)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))\n\n    print(\"--- STARTING FLORENCE-2 VIDEO INFERENCE ---\")\n    pbar = tqdm(total=total_frames)\n    \n    prompt = TASK_PROMPT + TEXT_INPUT\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret: break\n        \n        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        pil_image = Image.fromarray(image_rgb)\n        \n        # 1. VLM Inference\n        inputs = processor(text=prompt, images=pil_image, return_tensors=\"pt\").to(device, torch.float16)\n        \n        with torch.no_grad():\n            generated_ids = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                pixel_values=inputs[\"pixel_values\"],\n                max_new_tokens=1024,\n                do_sample=False,\n                num_beams=3\n            )\n            \n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        parsed_answer = processor.post_process_generation(generated_text, task=TASK_PROMPT, image_size=(w, h))\n        \n        # 2. Parse Polygons\n        pred_mask = np.zeros((h, w), dtype=np.uint8)\n        polygons_dict = parsed_answer.get(TASK_PROMPT, {})\n        polygons_list = polygons_dict.get('polygons', polygons_dict.get('Polygons', []))\n        \n        for obj_polys in polygons_list:\n            for poly in obj_polys:\n                if len(poly) >= 6:\n                    poly_np = np.array(poly).reshape(-1, 2).astype(np.int32)\n                    # Draw the polygon mask\n                    cv2.fillPoly(pred_mask, [poly_np], 1)\n                    # Draw a stark outline to show the VLM's anchor points\n                    cv2.polylines(frame, [poly_np], isClosed=True, color=(0, 255, 255), thickness=2)\n                    \n        # 3. Apply Visuals\n        res_frame = apply_overlay(frame, pred_mask)\n        cv2.putText(res_frame, \"Florence-2: Zero-Shot Text-to-Polygon\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n        \n        out.write(res_frame)\n        pbar.update(1)\n\n    cap.release()\n    out.release()\n    pbar.close()\n    print(f\"--- DONE. SAVED TO {OUTPUT_VIDEO} ---\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}