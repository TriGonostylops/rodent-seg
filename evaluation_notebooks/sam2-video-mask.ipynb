{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14792777,"datasetId":9457500,"databundleVersionId":15647062},{"sourceType":"datasetVersion","sourceId":14788594,"datasetId":9454374,"databundleVersionId":15642492}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/facebookresearch/sam2.git\n!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport shutil\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# Import Stage 1 Segformer for the initial prompt\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\n# Import SAM 2 Video Predictor\nfrom sam2.build_sam import build_sam2_video_predictor\n\n# ================= CONFIGURATION =================\n# Input / Output\nINPUT_VIDEO = \"/kaggle/input/datasets/gonoszgonosz/rat-test-video/test.mp4\"\nOUTPUT_VIDEO = \"/kaggle/working/SAM2_Tracked_Output.mp4\"\nTEMP_FRAME_DIR = \"/kaggle/working/sam2_temp_frames\"\n\n# Stage 1 Segformer (Auto-Prompter)\nSTAGE1_PATH = \"/kaggle/input/datasets/gonoszgonosz/b2-1024-weights/final_rat_model_b3_1024\"\n\n# SAM 2 Settings\nSAM2_CHECKPOINT = \"sam2_hiera_small.pt\"\nMODEL_CFG = \"sam2_hiera_s.yaml\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# =================================================\n\ndef extract_frames(video_path, output_dir):\n    \"\"\"Extracts all frames from a video as JPEGs for SAM 2 to read.\"\"\"\n    if os.path.exists(output_dir):\n        shutil.rmtree(output_dir)\n    os.makedirs(output_dir)\n    \n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        # SAM 2 expects sequentially numbered JPEGs\n        frame_name = f\"{frame_idx:05d}.jpg\"\n        cv2.imwrite(os.path.join(output_dir, frame_name), frame)\n        frame_idx += 1\n        \n    cap.release()\n    return fps, w, h, frame_idx\n\ndef get_stage1_bbox(image_path, model, processor):\n    \"\"\"Uses Stage 1 Segformer to find the rat in the first frame.\"\"\"\n    image = cv2.imread(image_path)\n    h, w = image.shape[:2]\n    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    inputs = processor(images=Image.fromarray(rgb_img), return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = torch.nn.functional.interpolate(\n            outputs.logits, size=(h, w), mode=\"bilinear\", align_corners=False\n        )\n        probs = torch.nn.functional.softmax(logits, dim=1)\n        mask = (probs[0, 1, :, :] > 0.5).cpu().numpy().astype(np.uint8)\n        \n    coords = np.column_stack(np.where(mask > 0))\n    if coords.size == 0:\n        return None\n        \n    y_min, x_min = coords.min(axis=0)\n    y_max, x_max = coords.max(axis=0)\n    return np.array([x_min, y_min, x_max, y_max], dtype=np.float32)\n\ndef apply_overlay(image, mask, color=(0, 0, 255), alpha=0.5):\n    \"\"\"Blends a solid color over the masked region.\"\"\"\n    overlay = np.full_like(image, color)\n    blended = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)\n    res = image.copy()\n    res[mask > 0] = blended[mask > 0]\n    return res\n\ndef main():\n    print(\"--- EXTRACTING FRAMES ---\")\n    fps, w, h, total_frames = extract_frames(INPUT_VIDEO, TEMP_FRAME_DIR)\n    print(f\"Extracted {total_frames} frames to {TEMP_FRAME_DIR}\")\n    \n    print(\"--- LOADING MODELS ---\")\n    # Load Stage 1\n    proc1 = SegformerImageProcessor.from_pretrained(STAGE1_PATH)\n    model1 = SegformerForSemanticSegmentation.from_pretrained(STAGE1_PATH).to(device)\n    model1.eval()\n    \n    # Load SAM 2 Video Predictor\n    predictor = build_sam2_video_predictor(MODEL_CFG, SAM2_CHECKPOINT, device=device)\n    \n    print(\"--- INITIALIZING SAM 2 MEMORY BANK ---\")\n    # SAM 2 scans the directory to build its internal temporal tensors\n    inference_state = predictor.init_state(video_path=TEMP_FRAME_DIR)\n    \n    print(\"--- GENERATING INITIAL PROMPT VIA STAGE 1 ---\")\n    first_frame_path = os.path.join(TEMP_FRAME_DIR, \"00000.jpg\")\n    init_box = get_stage1_bbox(first_frame_path, model1, proc1)\n    \n    if init_box is None:\n        print(\"CRITICAL ERROR: Stage 1 could not find the rat in Frame 0. Cannot prompt SAM 2.\")\n        return\n        \n    print(f\"Bounding Box Found: {init_box}\")\n    \n    # Send the bounding box to SAM 2 for Frame 0\n    # obj_id=1 represents the rat entity it needs to track\n    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n        inference_state=inference_state,\n        frame_idx=0,\n        obj_id=1,\n        box=init_box\n    )\n    \n    print(\"--- PROPAGATING THROUGH VIDEO ---\")\n    # Setup Video Writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_video = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))\n    \n    # SAM 2 yields the masks frame by frame as it calculates them\n    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n        frame_path = os.path.join(TEMP_FRAME_DIR, f\"{out_frame_idx:05d}.jpg\")\n        frame = cv2.imread(frame_path)\n        \n        # Extract the binary mask (logits > 0.0)\n        mask = (out_mask_logits[0] > 0.0).cpu().numpy().squeeze().astype(np.uint8)\n        \n        # Draw Output\n        res_frame = apply_overlay(frame, mask, color=(0, 255, 0)) # Green mask for SAM 2\n        \n        # Add labels\n        cv2.putText(res_frame, f\"SAM 2 Temporal Tracking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n        if out_frame_idx == 0:\n            # Draw the Stage 1 bounding box on the first frame so you can see the prompt\n            x1, y1, x2, y2 = map(int, init_box)\n            cv2.rectangle(res_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n            cv2.putText(res_frame, \"Stage 1 Auto-Prompt Box\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n            \n        out_video.write(res_frame)\n        \n        # Basic progress output\n        if out_frame_idx % 50 == 0:\n            print(f\"Processed frame {out_frame_idx}/{total_frames}\")\n\n    out_video.release()\n    print(f\"--- DONE. SAVED TO {OUTPUT_VIDEO} ---\")\n    \n    # Cleanup temporary frames\n    shutil.rmtree(TEMP_FRAME_DIR)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}